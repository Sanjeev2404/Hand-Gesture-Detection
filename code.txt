import cv2
import mediapipe as mp
import numpy as np
from google.colab import files
import matplotlib.pyplot as plt
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True,
min_detection_confidence=0.7)
mp_draw = mp.solutions.drawing_utils
def classify_gesture(landmarks):
thumb_tip = landmarks[4].y
index_tip = landmarks[8].y
middle_tip = landmarks[12].y
if index_tip < thumb_tip and middle_tip < thumb_tip:
return "âœŒï¸ Peace/Victory"
elif index_tip < thumb_tip:
return "â˜ï¸ Pointing"
else:
return "ðŸ‘ Thumbs Up (default)"
uploaded = files.upload()
for fn in uploaded.keys():
EXP.NO:8 MINI PROJECT: HAND GESTURE RECOGNITION

img = cv2.imread(fn)
rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
results = hands.process(rgb)
if results.multi_hand_landmarks:
for hand_landmarks in results.multi_hand_landmarks:
mp_draw.draw_landmarks(img, hand_landmarks,
mp_hands.HAND_CONNECTIONS)
gesture = classify_gesture(hand_landmarks.landmark)
print(f"{fn} â†’ {gesture}")
else:
print(f"{fn} â†’ No hand detected")

plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.axis("off")
plt.show()